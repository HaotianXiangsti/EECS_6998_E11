{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "64185fe67db14777879d9cb961425cb0": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4370fd084b80406a8a2c420cfaa6c057",
       "IPY_MODEL_e0a2aa054172402c83de548103dae7d8",
       "IPY_MODEL_dc0cabc1a6974f0fba723649faa491ed"
      ],
      "layout": "IPY_MODEL_696ba59a92c243ec978e09e659549bf9"
     }
    },
    "4370fd084b80406a8a2c420cfaa6c057": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8bf4f4bacf1a44aab3e8fbf680d98ca1",
      "placeholder": "​",
      "style": "IPY_MODEL_515d304780d74e9a9331922a528f4222",
      "value": "preprocessor_config.json: 100%"
     }
    },
    "e0a2aa054172402c83de548103dae7d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8c08653d1478484a978c5f73b7573c3c",
      "max": 538,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_26b929e7f9fa4ff4bd60fe8896a77914",
      "value": 538
     }
    },
    "dc0cabc1a6974f0fba723649faa491ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_feadc2fa4d7d458aa0825ec893f7cae2",
      "placeholder": "​",
      "style": "IPY_MODEL_1acf8aaa6cb646ccb3af7b2ea5a28512",
      "value": " 538/538 [00:00&lt;00:00, 37.8kB/s]"
     }
    },
    "696ba59a92c243ec978e09e659549bf9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bf4f4bacf1a44aab3e8fbf680d98ca1": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "515d304780d74e9a9331922a528f4222": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c08653d1478484a978c5f73b7573c3c": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "26b929e7f9fa4ff4bd60fe8896a77914": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "feadc2fa4d7d458aa0825ec893f7cae2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1acf8aaa6cb646ccb3af7b2ea5a28512": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df3bff1057804f4e900896c6f65106eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_82606841215140fa8b8fa1a30e2e7db9",
       "IPY_MODEL_8daa452ea5574b45b9335206e4a99c1b",
       "IPY_MODEL_142d8ce847944211a082a237baa494fb"
      ],
      "layout": "IPY_MODEL_21c952b9c8804b49bbe78e00536a9566"
     }
    },
    "82606841215140fa8b8fa1a30e2e7db9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_36b53dce062545d88a845125bd499da9",
      "placeholder": "​",
      "style": "IPY_MODEL_243e22fb99044d64ae3637cb20fc3aa7",
      "value": "config.json: 100%"
     }
    },
    "8daa452ea5574b45b9335206e4a99c1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_225e31bd63a94f3daed69db95b2626ea",
      "max": 9120,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bb7219394be34af0827748292d6c5621",
      "value": 9120
     }
    },
    "142d8ce847944211a082a237baa494fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b07a502b604d4d4db73ed62d060d9c9b",
      "placeholder": "​",
      "style": "IPY_MODEL_99d46e9aed2145dc9659b7f860f029f8",
      "value": " 9.12k/9.12k [00:00&lt;00:00, 797kB/s]"
     }
    },
    "21c952b9c8804b49bbe78e00536a9566": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "36b53dce062545d88a845125bd499da9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "243e22fb99044d64ae3637cb20fc3aa7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "225e31bd63a94f3daed69db95b2626ea": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb7219394be34af0827748292d6c5621": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b07a502b604d4d4db73ed62d060d9c9b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99d46e9aed2145dc9659b7f860f029f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f85e65067374c728e0f4bb1f58fc848": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_494390f6f95a4c5e8caa982bec90330b",
       "IPY_MODEL_08ae18297b7d46c9a41b7e09c0bfb859",
       "IPY_MODEL_5474306cda6a4b5cba8870f5d8162f68"
      ],
      "layout": "IPY_MODEL_dce73eb8fbfd44c1aaf09d63e9c6cbf3"
     }
    },
    "494390f6f95a4c5e8caa982bec90330b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b812b2921b543b9b0fbc3602f2ad990",
      "placeholder": "​",
      "style": "IPY_MODEL_4b100001bbce400f9af3cae91297f7cb",
      "value": "model.safetensors: 100%"
     }
    },
    "08ae18297b7d46c9a41b7e09c0bfb859": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c7e3c1aad6f41abb683fd1a13ac183b",
      "max": 431768064,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_265cef8fc0514eaea810e0d69ae18246",
      "value": 431768064
     }
    },
    "5474306cda6a4b5cba8870f5d8162f68": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8d1f634ae7a4822ab40d748c1b9d2be",
      "placeholder": "​",
      "style": "IPY_MODEL_c005217800e241deafd1565603cc1a96",
      "value": " 432M/432M [00:00&lt;00:00, 457MB/s]"
     }
    },
    "dce73eb8fbfd44c1aaf09d63e9c6cbf3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b812b2921b543b9b0fbc3602f2ad990": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b100001bbce400f9af3cae91297f7cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6c7e3c1aad6f41abb683fd1a13ac183b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "265cef8fc0514eaea810e0d69ae18246": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8d1f634ae7a4822ab40d748c1b9d2be": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c005217800e241deafd1565603cc1a96": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXLnfZIKDUBp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Omniglot\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "#from transformers import BertModel, BertConfig\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import ConcatDataset\n",
    "import torchvision.transforms\n",
    "import torch.utils.data as data\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score, precision_recall_curve\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import Omniglot\n",
    "from torchvision.models import resnet18\n",
    "from tqdm import tqdm\n",
    "import torch.utils.data as data\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve\n",
    "import glob\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "#from linformer import Linformer\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "#from utils import *\n",
    "import logging\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import configparser\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from skimage import io, transform\n",
    "from rasterio.features import rasterize\n",
    "from shapely.ops import cascaded_union, unary_union\n",
    "from shapely.geometry import Polygon\n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "import pickle\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from torch.autograd import Variable\n",
    "from pytorch_metric_learning import losses\n",
    "import copy\n",
    "\n",
    "import albumentations as A\n",
    "from collections import OrderedDict, defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import timm\n",
    "from torch.cuda import amp\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from time import time\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning)"
   ],
   "metadata": {
    "id": "wFbpJoPTFMKh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df = pd.read_csv(r'C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\ForestNetDataset\\train.csv')\n",
    "val_df = pd.read_csv(r'C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\ForestNetDataset\\val.csv')\n",
    "test_df = pd.read_csv(r'C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\ForestNetDataset\\test.csv')\n",
    "\n",
    "train_df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZVxMAXs9FzmS",
    "outputId": "001f6eed-e29e-4b61-b69c-594b5b6b75fe"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_df_class = train_df.groupby(['merged_label']).size().to_frame('train').reset_index()\n",
    "val_df_class = val_df.groupby(['merged_label']).size().to_frame('val').reset_index()\n",
    "test_df_class = test_df.groupby(['merged_label']).size().to_frame('test').reset_index()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "x_axis = np.arange(len(train_df_class['merged_label']))\n",
    "index = [[train_df_class,'train',-0.2],[val_df_class,'val',0],[test_df_class,'test',0.2]]\n",
    "for i in index:\n",
    "  df = i[0]\n",
    "  bars = plt.bar(x_axis + i[2], df[i[1]] , width=0.2, label=i[1])\n",
    "  for bar in bars:\n",
    "      yval = bar.get_height()\n",
    "      plt.text(bar.get_x(), yval + .005, yval)\n",
    "plt.xticks(x_axis,train_df_class['merged_label'])\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "id": "yEIs4-u8F4n-",
    "outputId": "4388695b-cef3-4e68-8185-d9315f207ce9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class ForestDataset(Dataset):\n",
    "    \"\"\"Forest dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file_to_df, root_dir=r'C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\ForestNetDataset', transform=None, types=\"classifier\", device = device):\n",
    "        self.csv = csv_file_to_df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.label_to_int = {'Grassland shrubland': torch.asarray([1,0,0,0]), 'Other': torch.asarray([0,1,0,0]), 'Plantation': torch.asarray([0,0,1,0]), 'Smallholder agriculture': torch.asarray([0,0,0,1])}\n",
    "        self.types = types\n",
    "        self.device =device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def poly_from_utm(self, polygon):\n",
    "        poly_pts = []\n",
    "\n",
    "        poly = unary_union(polygon)\n",
    "        for i in np.array(poly.exterior.coords):\n",
    "\n",
    "            poly_pts.append(tuple(i))\n",
    "\n",
    "        new_poly = Polygon(poly_pts)\n",
    "        return new_poly\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        label = self.csv.iloc[idx, 0]\n",
    "        merged_label = self.csv.iloc[idx, 1]\n",
    "        lat = self.csv.iloc[idx, 2]\n",
    "        long = self.csv.iloc[idx, 3]\n",
    "        year = self.csv.iloc[idx, 4]\n",
    "        folder = self.csv.iloc[idx, 5]\n",
    "\n",
    "        ## Load the image and auxiliary\n",
    "        image = Image.open(f'{self.root_dir}/{folder}/images/visible/composite.png')#io.imread(f'{self.root_dir}/{folder}/images/visible/composite.png')\n",
    "        image = self.transform(image)\n",
    "        #slope = np.load(f'{self.root_dir}/{folder}/auxiliary/slope.npy')\n",
    "\n",
    "        ## Get the segmentation map\n",
    "        with open(f'{self.root_dir}/{folder}/forest_loss_region.pkl', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        nx, ny = 332, 332\n",
    "        xy_array = np.empty((0,2))\n",
    "        if data.geom_type == 'Polygon':\n",
    "            data = [data]\n",
    "        elif data.geom_type == 'Multipolygon':\n",
    "            data = list(data)\n",
    "\n",
    "        poly_shp = []\n",
    "        for poly_verts in data:\n",
    "            poly_shp.append(self.poly_from_utm(poly_verts))\n",
    "\n",
    "        mask = rasterize(shapes=poly_shp, out_shape=(332,332))\n",
    "        seg = np.array(mask)\n",
    "\n",
    "        '''\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "        '''\n",
    "\n",
    "\n",
    "        image = image.to(self.device, dtype=torch.float32)#torch.from_numpy(image).to(self.device, dtype=torch.float32)\n",
    "        #image = image.permute(2, 0, 1)\n",
    "        seg = torch.from_numpy(seg).to(self.device, dtype=torch.float32)\n",
    "        #slope = torch.from_numpy(slope).type(torch.float)\n",
    "\n",
    "        two_channel_seg = torch.zeros((2, seg.shape[0], seg.shape[1]), dtype=torch.float).to(self.device, dtype=torch.float32)\n",
    "\n",
    "        # 将单通道掩码映射到两个通道\n",
    "        two_channel_seg[0, :, :] = (seg == 0).float()  # 耕地的像素设为第一个通道的值\n",
    "        two_channel_seg[1, :, :] = (seg == 1).float()  # 森林的像素设为第二个通道的值\n",
    "\n",
    "\n",
    "        merged_label = self.label_to_int[merged_label]\n",
    "\n",
    "        merged_label = merged_label.to(self.device, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "        #image = image[:, 86:246, 86:246]\n",
    "        seg = two_channel_seg[:, 52:276, 52:276]#seg[86:246, 86:246]\n",
    "        #slope = slope[86:246, 86:246]\n",
    "        if self.types == \"classifier\":\n",
    "            return image, merged_label, seg\n",
    "        else:\n",
    "            return image, merged_label, seg\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "])"
   ],
   "metadata": {
    "id": "5BEcpN6TGM6l"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "a = np.load(r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\ForestNetDataset\\examples\\-0.002226324002905_109.97159881327198\\auxiliary\\srtm.npy\")\n",
    "plt.imshow(a.transpose(1,2,0))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch.multiprocessing as mp\n",
    "\n",
    "# 设置 multiprocessing 启动方法为 'spawn'\n",
    "mp.set_start_method('spawn', force=True)\n"
   ],
   "metadata": {
    "id": "xGt1Rsr4sZVz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def f_score(gt,pr, beta=1, eps=1e-7, threshold=None, activation=\"None\"):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        pr (torch.Tensor): A list of predicted elements\n",
    "        gt (torch.Tensor):  A list of elements that are to be predicted\n",
    "        eps (float): epsilon to avoid zero division\n",
    "        threshold: threshold for outputs binarization\n",
    "    Returns:\n",
    "        float: IoU (Jaccard) score\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "    if threshold is not None:\n",
    "        pr = (pr > threshold).float()\n",
    "    '''\n",
    "\n",
    "    beta = 1\n",
    "    gt = torch.argmax(gt, dim =1).float()\n",
    "    pr = torch.argmax(pr, dim = 1).float()\n",
    "    tp = torch.sum(gt * pr)\n",
    "    fp = torch.sum(pr) - tp\n",
    "    fn = torch.sum(gt) - tp\n",
    "\n",
    "    score = ((1 + beta ** 2) * tp + eps) \\\n",
    "            / ((1 + beta ** 2) * tp + beta ** 2 * fn + fp + eps)\n",
    "\n",
    "    return 1 - score\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    __name__ = 'dice_loss'\n",
    "\n",
    "    def __init__(self, eps=1e-7, activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self,y_gt, y_pr):\n",
    "        return 1 - f_score(y_pr, y_gt, beta=1.,\n",
    "                           eps=self.eps, threshold=None,\n",
    "                           activation=self.activation)\n",
    "\n",
    "\n",
    "class BCEDiceLoss(DiceLoss):\n",
    "    __name__ = 'bce_dice_loss'\n",
    "\n",
    "    def __init__(self, eps=1e-7, activation='None', lambda_dice=1.0, lambda_bce=1.0):\n",
    "        super().__init__(eps, activation)\n",
    "        if activation == None:\n",
    "            self.bce = nn.CrossEntropyLoss(reduction='mean')\n",
    "        else:\n",
    "            self.bce = nn.BCEWithLogitsLoss(reduction='mean')\n",
    "        self.lambda_dice=lambda_dice\n",
    "        self.lambda_bce=lambda_bce\n",
    "\n",
    "    def forward(self, y_gt, y_pr):\n",
    "        dice = super().forward(y_pr, y_gt)\n",
    "        bce = self.bce(y_pr, y_gt)\n",
    "        return (self.lambda_dice*dice) + (self.lambda_bce* bce), dice"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def f1_loss(labels, outputs):\n",
    "    # 计算True Positives (TP)、False Positives (FP)和False Negatives (FN)\n",
    "    TP = torch.sum(outputs[:, 1] * labels[:, 1])  # True Positives\n",
    "    FP = torch.sum(outputs[:, 1] * (1 - labels[:, 1]))  # False Positives\n",
    "    FN = torch.sum((1 - outputs[:, 1]) * labels[:, 1])  # False Negatives\n",
    "\n",
    "    # 计算精确度 (Precision) 和召回率 (Recall)\n",
    "    precision = TP / (TP + FP + 1e-6)  # Precision\n",
    "    recall = TP / (TP + FN + 1e-6)  # Recall\n",
    "\n",
    "    # 计算F1 Score\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-6)  # F1 Score\n",
    "\n",
    "    # 计算F1 Loss\n",
    "    f1_loss = 1 - f1  # F1 Loss\n",
    "\n",
    "    return f1_loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "SMOOTH = 1e-6\n",
    "\n",
    "def iou_pytorch(labels, outputs):\n",
    "    # You can comment out this line if you are passing tensors of equal shape\n",
    "    # But if you are passing output from UNet or something it will most probably\n",
    "    # be with the BATCH x 1 x H x W shape\n",
    "    outputs = torch.argmax(outputs, dim=1).squeeze(1)\n",
    "    labels = torch.argmax(labels, dim=1).squeeze(1)\n",
    "\n",
    "\n",
    "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
    "    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n",
    "\n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
    "\n",
    "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
    "\n",
    "    return 1 - iou.mean()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Currently Not Used\n",
    "\n",
    "#from transformers import Mask2FormerConfig, AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "import torchvision.models\n",
    "vgg16 = torchvision.models.vgg16(pretrained=True).to(device).features[:29]\n",
    "vgg16.eval()\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "nbClass = 4\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "class UNetDecoder(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNetDecoder, self).__init__()\n",
    "\n",
    "        # 解码器部分\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, 64, kernel_size=4, stride=4),  # 上采样\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=4, padding=0),  # 上采样操作\n",
    "            nn.Softmax2d()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 解码器部分\n",
    "        output = self.decoder(x)\n",
    "        return output\n",
    "\n",
    "# 创建模型\n",
    "in_channels = 512\n",
    "out_channels = 2\n",
    "decoder = UNetDecoder(in_channels, out_channels).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-3) #, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "\n",
    "number_of_epochs = 100\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"total_run\"\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    pbar = tqdm(trainloader)\n",
    "    train_loss = 0\n",
    "    for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "      #Feature_Extraction_Model.eval()\n",
    "      #inputs = processor(images=images, return_tensors=\"pt\")\n",
    "      vgg16.eval()\n",
    "      with torch.no_grad():\n",
    "          outputs = vgg16(images)\n",
    "\n",
    "      #upsampled_features = outputs#.pixel_decoder_last_hidden_state\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      output_final = decoder(outputs)\n",
    "\n",
    "      loss = criterion(seg.type(torch.float32), output_final.type(torch.float32))\n",
    "\n",
    "      tp = f1_loss(seg, output_final) #iou_pytorch(seg, output_final)\n",
    "      iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "      loss = loss+tp\n",
    "\n",
    "      pbar.set_postfix(Train_Loss=loss.item(), Train_F1_Loss = tp.item(), IOU =iou.item())\n",
    "\n",
    "      train_loss += loss\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(valloader)\n",
    "        val_loss = 0\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "            #Feature_Extraction_Model.eval()\n",
    "            #inputs = processor(images=images, return_tensors=\"pt\")\n",
    "            vgg16.eval()\n",
    "            with torch.no_grad():\n",
    "                outputs = vgg16(images)\n",
    "\n",
    "            #upsampled_features = outputs#.pixel_decoder_last_hidden_state\n",
    "\n",
    "\n",
    "            output_final = decoder(outputs)\n",
    "\n",
    "            loss = criterion(seg.type(torch.float32), output_final.type(torch.float32))\n",
    "            tp = f1_loss(seg, output_final)#iou_pytorch(seg, output_final)\n",
    "            iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "            loss = loss+tp\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), VAL_F1_Loss = tp.item(), IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(decoder.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt.pt\"))\n",
    "\n",
    "        plt.imshow(torch.argmax(output_final, dim = 1)[0].detach().cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    \"\"\"(conv => BN => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(nn.MaxPool2d(2), double_conv(in_ch, out_ch))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.mpconv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch // 2, in_ch // 2, 2, stride=2)\n",
    "\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, (diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2))\n",
    "\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64, 128)\n",
    "        self.down2 = down(128, 256)\n",
    "        self.down3 = down(256, 512)\n",
    "        self.down4 = down(512, 512)\n",
    "        self.up1 = up(1024, 256, False)\n",
    "        self.up2 = up(512, 128, False)\n",
    "        self.up3 = up(256, 64, False)\n",
    "        self.up4 = up(128, 64, False)\n",
    "        self.outc = outconv(64, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        x = self.outc(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "model = UNet(n_channels=3, n_classes=2).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Segmentation\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义分割模型\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        # 创建timm模型\n",
    "        model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "        # 去除最后的全局平均池化层和全连接层\n",
    "        self.backbone = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "        # 获取num_features\n",
    "        num_features = model.layer4[2].conv3.out_channels\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_features, 512, kernel_size=7, padding=3, output_padding=0, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=8, padding=1, output_padding=0, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=8, padding=1, output_padding=0, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, num_classes, kernel_size=8, padding=3, output_padding=0, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.segmentation_head(x)\n",
    "        return x\n",
    "\n",
    "# 指定模型名称和类别数\n",
    "model_name = 'resnet50'  # 选择适合你任务的模型，可以根据需要更改\n",
    "num_classes = 2  # 类别数为2\n",
    "\n",
    "# 创建分割模型\n",
    "#segmentation_model = SegmentationModel(model_name, num_classes).to(device)\n",
    "\n",
    "\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "\n",
    "\n",
    "number_of_epochs = 1000\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"Unet_Version\"\n",
    "\n",
    "#ckpt_path = os.path.join(\"Model_Ckpts\",runs+f\"ckpt_2.pt\")\n",
    "\n",
    "#ckpt_for_tr = r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\Model_Ckpts\\Unet_Versionckpt_test_for_seg_new_2_tr.pth\" #os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\")\n",
    "\n",
    "#checkpoint = torch.load(ckpt_for_tr)\n",
    "\n",
    "#cls_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#cls_model.train()\n",
    "\n",
    "#model_vit.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#segmentation_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)#, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.MSELoss() #nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "#scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 51, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "#model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "MSE_list = list()\n",
    "\n",
    "F1_list =list()\n",
    "\n",
    "IOU_list = list()\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    pbar = tqdm(trainloader)\n",
    "    train_loss = 0\n",
    "    acc = 0\n",
    "    model.train()\n",
    "\n",
    "    for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = model(images)#(images, output_seg)#(output_final)\n",
    "\n",
    "      loss = criterion(seg, outputs)\n",
    "\n",
    "      outputs = torch.nn.functional.softmax(outputs, dim = 1)\n",
    "\n",
    "\n",
    "      tp = f1_loss(seg, outputs) #iou_pytorch(seg, output_final)\n",
    "\n",
    "      iou = iou_pytorch(seg, outputs)\n",
    "\n",
    "      #loss = loss+tp\n",
    "\n",
    "      MSE_list.append(loss.item)\n",
    "\n",
    "      F1_list.append(tp.item())\n",
    "\n",
    "      IOU_list.append(iou.item())\n",
    "\n",
    "      pbar.set_postfix(Train_Loss=loss.item(), Train_F1_Loss = tp.item(), IOU =iou.item())#, Train_Accuracy = accuracy, Train_F1_Score = f1)#, Train_F1_Loss = tp.item(), IOU =iou.item())\n",
    "\n",
    "      train_loss += loss\n",
    "\n",
    "\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      '''\n",
    "      if train_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = train_loss\n",
    "\n",
    "\n",
    "                checkpoint = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "      '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(valloader)\n",
    "        val_loss = 0\n",
    "        val_acc_mean = 0\n",
    "\n",
    "        #cls_model.eval()\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "            outputs = model(images)#(images, output_seg)#(output_final)\n",
    "\n",
    "            loss = criterion(seg, outputs)\n",
    "\n",
    "            outputs = torch.nn.functional.softmax(outputs, dim = 1)\n",
    "\n",
    "            tp = f1_loss(seg, outputs)#iou_pytorch(seg, output_final)\n",
    "\n",
    "            iou = iou_pytorch(seg, outputs)\n",
    "\n",
    "            #loss = loss+tp\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), VAL_F1_Loss = tp.item(), IOU = iou.item())#, Val_Accuracy = accuracy, Val_F1_Score = f1) #, VAL_F1_Loss = tp.item(), IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n",
    "\n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "\n",
    "                checkpoint_vit = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint_vit, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_test_for_seg_new_21_12.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"###### Epoch: \", epoch,\"_\",val_acc_mean/15,\" ######\")\n",
    "        #plt.imshow(torch.argmax(output_final, dim = 1)[0].detach().cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        #x= self.softmax(x)\n",
    "        return x #torch.sigmoid(x)\n",
    "\n",
    "model_vit = ViT(\n",
    "    image_size=(224, 224),\n",
    "    patch_size=(16, 16),\n",
    "    num_classes=4,  # 输出类别数\n",
    "    dim=128,         # 模型维度\n",
    "    depth=6,         # Transformer层的深度\n",
    "    heads=8,         # 注意力头的数量\n",
    "    mlp_dim=256,     # MLP的隐藏层维度\n",
    "    pool='cls',      # 池化方式，可以是'cls'或'mean'\n",
    "    channels=3,  # 输入图像通道数\n",
    "    dim_head=64,     # 注意力头的维度\n",
    "    dropout=0,     # Dropout概率\n",
    "    emb_dropout=0  # 嵌入层的Dropout概率\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Drivers Classification\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "\n",
    "\n",
    "number_of_epochs = 10000\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"Unet_Version\"\n",
    "\n",
    "#ckpt_path = os.path.join(\"Model_Ckpts\",runs+f\"ckpt_2.pt\")\n",
    "\n",
    "ckpt_for_tr = r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\Model_Ckpts\\Unet_Versionckpt_test_less_lr_model_optimizer_scheduler_checkpoint_new.pth\" #os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\")\n",
    "\n",
    "checkpoint = torch.load(ckpt_for_tr)\n",
    "\n",
    "#cls_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#cls_model.train()\n",
    "\n",
    "#model_vit.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model_vit.parameters(), lr=1e-3)#, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 51, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "#model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "Total_Cross_Entropy_List =list()\n",
    "\n",
    "Total_Accuracy_List = list()\n",
    "\n",
    "Total_F1_Score_List = list()\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    pbar = tqdm(trainloader)\n",
    "    train_loss = 0\n",
    "    acc = 0\n",
    "    model_vit.train()\n",
    "\n",
    "    Cross_Entropy_List =list()\n",
    "\n",
    "    Accuracy_List = list()\n",
    "\n",
    "    F1_Score_List = list()\n",
    "\n",
    "    for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      zero_input = torch.empty(seg.shape[0], 1, seg.shape[2], seg.shape[-1])\n",
    "\n",
    "      add_zero = torch.zeros_like(zero_input).to(device)\n",
    "\n",
    "      output_seg = torch.concat((seg, add_zero), dim = 1)\n",
    "\n",
    "      output_final = images #+ output_seg\n",
    "\n",
    "      outputs = model_vit(output_final)#(images, output_seg)#(output_final)\n",
    "\n",
    "      loss = criterion(labels, outputs)\n",
    "\n",
    "      #tp = f1_loss(seg, output_final) #iou_pytorch(seg, output_final)\n",
    "\n",
    "      #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "      #loss = loss+tp\n",
    "      outputs = torch.nn.functional.softmax(outputs, dim = -1)\n",
    "      outputs = torch.argmax(outputs,dim= -1).squeeze()\n",
    "      labels = torch.argmax(labels,dim= -1).squeeze()\n",
    "\n",
    "      correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "      total = labels.size(0)\n",
    "\n",
    "\n",
    "      accuracy = correct / total\n",
    "\n",
    "      f1 = f1_score(labels.detach().cpu().numpy(), outputs.detach().cpu().numpy(), average='macro')\n",
    "\n",
    "      pbar.set_postfix(Train_Loss=loss.item(), Train_Accuracy = accuracy, Train_F1_Score = f1)#, Train_F1_Loss = tp.item(), IOU =iou.item())\n",
    "\n",
    "      Cross_Entropy_List.append(loss.item())\n",
    "\n",
    "      Accuracy_List.append(accuracy)\n",
    "\n",
    "      F1_Score_List.append(f1)\n",
    "\n",
    "      train_loss += loss\n",
    "\n",
    "\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      '''\n",
    "      if train_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = train_loss\n",
    "\n",
    "\n",
    "                checkpoint = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "      '''\n",
    "\n",
    "    Total_Cross_Entropy_List.append(np.array(Cross_Entropy_List).mean())\n",
    "\n",
    "    Total_Accuracy_List.append(np.array(Accuracy_List).mean())\n",
    "\n",
    "    Total_F1_Score_List.append(np.array(F1_Score_List).mean())\n",
    "\n",
    "    print(len(Total_Cross_Entropy_List))\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(valloader)\n",
    "        val_loss = 0\n",
    "        val_acc_mean = 0\n",
    "\n",
    "        #cls_model.eval()\n",
    "\n",
    "        model_vit.eval()\n",
    "\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "            zero_input = torch.empty(seg.shape[0], 1, seg.shape[2], seg.shape[-1])\n",
    "\n",
    "            add_zero = torch.zeros_like(zero_input).to(device)\n",
    "\n",
    "            output_seg = torch.concat((seg, add_zero), dim = 1)\n",
    "\n",
    "            output_final = images #+ output_seg\n",
    "\n",
    "            outputs = model_vit(output_final)#(images, output_seg)#(output_final)\n",
    "\n",
    "            loss = criterion(labels, outputs)\n",
    "\n",
    "            #tp = f1_loss(seg, output_final)#iou_pytorch(seg, output_final)\n",
    "\n",
    "            #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "            #loss = loss+tp\n",
    "\n",
    "            outputs = torch.nn.functional.softmax(outputs, dim = -1)\n",
    "\n",
    "            outputs = torch.argmax(outputs,dim=-1).squeeze()\n",
    "            labels = torch.argmax(labels,dim=-1).squeeze()\n",
    "\n",
    "            correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "            total = labels.size(0)\n",
    "\n",
    "\n",
    "\n",
    "            accuracy = correct / total\n",
    "\n",
    "            f1 = f1_score(labels.detach().cpu().numpy(), outputs.detach().cpu().numpy(), average='macro')\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), Val_Accuracy = accuracy, Val_F1_Score = f1) #, VAL_F1_Loss = tp.item(), IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n",
    "\n",
    "            val_acc_mean += accuracy\n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "\n",
    "                checkpoint_vit = {\n",
    "                                'model_state_dict': model_vit.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint_vit, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_test_less_lr_model_optimizer_scheduler_1221_plain_VIT.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"###### Epoch: \", epoch,\"_\",val_acc_mean/15,\" ######\")\n",
    "        #plt.imshow(torch.argmax(output_final, dim = 1)[0].detach().cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(F1_Score_List)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T21:40:15.760253800Z",
     "start_time": "2023-12-22T21:40:15.728569600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x2260800ef10>]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(F1_Score_List)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-22T21:40:19.255242700Z",
     "start_time": "2023-12-22T21:40:19.098408100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_q=160, dim_k=160, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inner_dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm_q = nn.LayerNorm(dim_q)\n",
    "        self.norm_k = nn.LayerNorm(dim_k)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_q = nn.Linear(dim_q, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(dim_k, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(dim_k, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim_q),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, q, k):\n",
    "        q = self.norm_q(q)\n",
    "        k = self.norm_k(k)\n",
    "\n",
    "        q, k, v = self.to_q(q), self.to_k(k), self.to_v(k)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', channels = 3, dim_head = 64, dropout = 0., emb_dropout = 0.):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, img, info):\n",
    "        x = self.to_patch_embedding(img)\n",
    "        x_k = info\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x, x_k)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        x= self.softmax(x)\n",
    "        return x #torch.sigmoid(x)\n",
    "\n",
    "model_vit = ViT(\n",
    "    image_size=(160, 160),\n",
    "    patch_size=(16, 16),\n",
    "    num_classes=3,  # 输出类别数\n",
    "    dim=128,         # 模型维度\n",
    "    depth=6,         # Transformer层的深度\n",
    "    heads=8,         # 注意力头的数量\n",
    "    mlp_dim=256,     # MLP的隐藏层维度\n",
    "    pool='cls',      # 池化方式，可以是'cls'或'mean'\n",
    "    channels=3,  # 输入图像通道数\n",
    "    dim_head=64,     # 注意力头的维度\n",
    "    dropout=0,     # Dropout概率\n",
    "    emb_dropout=0  # 嵌入层的Dropout概率\n",
    ").to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_q, dim_k, heads=8, dim_head=64, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inner_dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm_q = nn.LayerNorm(dim_q)\n",
    "        self.norm_k = nn.LayerNorm(dim_k)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_q = nn.Linear(dim_q, inner_dim, bias=False)\n",
    "        self.to_k = nn.Linear(dim_k, inner_dim, bias=False)\n",
    "        self.to_v = nn.Linear(dim_k, inner_dim, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim_q),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, q, k):\n",
    "        q = self.norm_q(q)\n",
    "        k = self.norm_k(k)\n",
    "\n",
    "        q, k, v = self.to_q(q), self.to_k(k), self.to_v(k)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        return self.to_out(out)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "\n",
    "        # 第一个卷积层\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 第二个卷积层\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(32 * 40 * 40, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 4)  # 4 是输出类别数量\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # 展平\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.fc2(x)\n",
    "        x= self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# 创建模型实例\n",
    "cls_model = CNNClassifier().to(device)\n",
    "\n",
    "\n",
    "\n",
    "# 输出形状为 (batch_size, 4)，表示每个样本对应的类别分数\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class ModifiedResNet50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ModifiedResNet50, self).__init__()\n",
    "        self.resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "        # 修改第一个卷积层以适应5个输入通道\n",
    "        self.resnet50.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        # 更新最后一层以适应你的任务\n",
    "        self.resnet50.fc = nn.Linear(2048, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet50(x)\n",
    "\n",
    "# 创建一个输入通道为5，输出类别为num_classes的模型\n",
    "num_classes = 4  # 替换为你的类别数量\n",
    "cls_model = ModifiedResNet50(num_classes)\n",
    "\n",
    "cls_model = nn.Sequential(\n",
    "    cls_model,\n",
    "    nn.Softmax(dim = 1)  # 添加 sigmoid 层\n",
    ").to(device)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    "\n",
    "# 使用 timm 创建一个预训练的 ResNet-18 模型\n",
    "cls_model = timm.create_model('resnet50', pretrained=True)\n",
    "\n",
    "# 获取原始模型的分类器中的输入特征数\n",
    "num_ftrs = cls_model.get_classifier().in_features\n",
    "\n",
    "# 修改模型的第一层卷积层，将输入通道数从3更改为5\n",
    "cls_model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "# 修改模型的全连接层，以适应你的任务（这里假设输出类别数为4）\n",
    "cls_model.fc = nn.Linear(num_ftrs, 4)\n",
    "\n",
    "cls_model = nn.Sequential(\n",
    "    cls_model,\n",
    "    nn.Softmax(dim = 1)  # 添加 sigmoid 层\n",
    ").to(device)\n",
    "\n",
    "cls_model = cls_model.to(device)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "timm.list_models()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "# 指定模型名称和预训练权重\n",
    "model_name = 'mobilevitv2_050' #'vit_base_patch16_224'  # 选择适合你任务的模型\n",
    "pretrained = True  # 使用预训练权重\n",
    "\n",
    "# 创建ViT模型\n",
    "timm_vit_model = timm.create_model(model_name, pretrained=pretrained, num_classes=4).to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Drivers Classification\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "\n",
    "\n",
    "number_of_epochs = 10000\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"Unet_Version\"\n",
    "\n",
    "#ckpt_path = os.path.join(\"Model_Ckpts\",runs+f\"ckpt_2.pt\")\n",
    "\n",
    "ckpt_for_tr = r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\Model_Ckpts\\Unet_Versionckpt_test_less_lr_model_optimizer_scheduler_checkpoint_new.pth\" #os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\")\n",
    "\n",
    "checkpoint = torch.load(ckpt_for_tr)\n",
    "\n",
    "#cls_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#cls_model.train()\n",
    "\n",
    "#model_vit.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(timm_vit_model.parameters(), lr=1e-3)#, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 51, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "#model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    pbar = tqdm(trainloader)\n",
    "    train_loss = 0\n",
    "    acc = 0\n",
    "    timm_vit_model.train()\n",
    "\n",
    "    for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      zero_input = torch.empty(seg.shape[0], 1, seg.shape[2], seg.shape[-1])\n",
    "\n",
    "      add_zero = torch.zeros_like(zero_input).to(device)\n",
    "\n",
    "      output_seg = torch.concat((seg, add_zero), dim = 1)\n",
    "\n",
    "      output_final = images #+ output_seg\n",
    "\n",
    "      outputs = timm_vit_model(output_final)#(images, output_seg)#(output_final)\n",
    "\n",
    "      loss = criterion(labels, outputs)\n",
    "\n",
    "      #tp = f1_loss(seg, output_final) #iou_pytorch(seg, output_final)\n",
    "\n",
    "      #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "      #loss = loss+tp\n",
    "      outputs = torch.nn.functional.softmax(outputs, dim = -1)\n",
    "      outputs = torch.argmax(outputs,dim= -1).squeeze()\n",
    "      labels = torch.argmax(labels,dim= -1).squeeze()\n",
    "\n",
    "      correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "      total = labels.size(0)\n",
    "\n",
    "\n",
    "      accuracy = correct / total\n",
    "\n",
    "      f1 = f1_score(labels.detach().cpu().numpy(), outputs.detach().cpu().numpy(), average='macro')\n",
    "\n",
    "      pbar.set_postfix(Train_Loss=loss.item(), Train_Accuracy = accuracy, Train_F1_Score = f1)#, Train_F1_Loss = tp.item(), IOU =iou.item())\n",
    "\n",
    "      train_loss += loss\n",
    "\n",
    "\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      '''\n",
    "      if train_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = train_loss\n",
    "\n",
    "\n",
    "                checkpoint = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "      '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(valloader)\n",
    "        val_loss = 0\n",
    "        val_acc_mean = 0\n",
    "\n",
    "        #cls_model.eval()\n",
    "\n",
    "        timm_vit_model.eval()\n",
    "\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "            zero_input = torch.empty(seg.shape[0], 1, seg.shape[2], seg.shape[-1])\n",
    "\n",
    "            add_zero = torch.zeros_like(zero_input).to(device)\n",
    "\n",
    "            output_seg = torch.concat((seg, add_zero), dim = 1)\n",
    "\n",
    "            output_final = images #+ output_seg\n",
    "\n",
    "            outputs = timm_vit_model(output_final)#(images, output_seg)#(output_final)\n",
    "\n",
    "            loss = criterion(labels, outputs)\n",
    "\n",
    "            #tp = f1_loss(seg, output_final)#iou_pytorch(seg, output_final)\n",
    "\n",
    "            #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "            #loss = loss+tp\n",
    "\n",
    "            outputs = torch.nn.functional.softmax(outputs, dim = -1)\n",
    "\n",
    "            outputs = torch.argmax(outputs,dim=-1).squeeze()\n",
    "            labels = torch.argmax(labels,dim=-1).squeeze()\n",
    "\n",
    "            correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "            total = labels.size(0)\n",
    "\n",
    "\n",
    "\n",
    "            accuracy = correct / total\n",
    "\n",
    "            f1 = f1_score(labels.detach().cpu().numpy(), outputs.detach().cpu().numpy(), average='macro')\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), Val_Accuracy = accuracy, Val_F1_Score = f1) #, VAL_F1_Loss = tp.item(), IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n",
    "\n",
    "            val_acc_mean += accuracy\n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "\n",
    "                checkpoint_vit = {\n",
    "                                'model_state_dict': timm_vit_model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint_vit, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_test_less_lr_model_optimizer_scheduler_checkpoint_new_seperate_img_and_seg.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"###### Epoch: \", epoch,\"_\",val_acc_mean/15,\" ######\")\n",
    "        #plt.imshow(torch.argmax(output_final, dim = 1)[0].detach().cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Segmentation\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义分割模型\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        # 创建timm模型\n",
    "        model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "        # 去除最后的全局平均池化层和全连接层\n",
    "        self.backbone = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "        # 获取num_features\n",
    "        num_features = model.layer4[2].conv3.out_channels\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_features, 512, kernel_size=7, padding=3, output_padding=0, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=8, padding=1, output_padding=0, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=8, padding=1, output_padding=0, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, num_classes, kernel_size=8, padding=3, output_padding=0, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.segmentation_head(x)\n",
    "        return x\n",
    "\n",
    "# 指定模型名称和类别数\n",
    "model_name = 'resnet50'  # 选择适合你任务的模型，可以根据需要更改\n",
    "num_classes = 2  # 类别数为2\n",
    "\n",
    "# 创建分割模型\n",
    "segmentation_model = SegmentationModel(model_name, num_classes).to(device)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "\n",
    "\n",
    "number_of_epochs = 10000\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"Unet_Version\"\n",
    "\n",
    "#ckpt_path = os.path.join(\"Model_Ckpts\",runs+f\"ckpt_2.pt\")\n",
    "\n",
    "ckpt_for_tr = r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\Model_Ckpts\\Unet_Versionckpt_test_for_seg_new_2_tr.pth\" #os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\")\n",
    "\n",
    "checkpoint = torch.load(ckpt_for_tr)\n",
    "\n",
    "#cls_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#cls_model.train()\n",
    "\n",
    "#model_vit.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "segmentation_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(segmentation_model.parameters(), lr=1e-3)#, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.MSELoss() #nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 51, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "#model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    pbar = tqdm(valloader)\n",
    "    train_loss = 0\n",
    "    acc = 0\n",
    "    segmentation_model.train()\n",
    "\n",
    "    for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      outputs = segmentation_model(images)#(images, output_seg)#(output_final)\n",
    "\n",
    "      loss = criterion(seg, outputs)\n",
    "\n",
    "      outputs = torch.nn.functional.softmax(outputs, dim = 1)\n",
    "\n",
    "\n",
    "      tp = f1_loss(seg, outputs) #iou_pytorch(seg, output_final)\n",
    "\n",
    "      iou = iou_pytorch(seg, outputs)\n",
    "\n",
    "      #loss = loss+tp\n",
    "\n",
    "\n",
    "      pbar.set_postfix(Train_Loss=loss.item(), Train_F1_Loss = tp.item(), IOU =iou.item())#, Train_Accuracy = accuracy, Train_F1_Score = f1)#, Train_F1_Loss = tp.item(), IOU =iou.item())\n",
    "\n",
    "      train_loss += loss\n",
    "\n",
    "\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      '''\n",
    "      if train_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = train_loss\n",
    "\n",
    "\n",
    "                checkpoint = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "      '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(valloader)\n",
    "        val_loss = 0\n",
    "        val_acc_mean = 0\n",
    "\n",
    "        #cls_model.eval()\n",
    "\n",
    "        segmentation_model.eval()\n",
    "\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "            outputs = segmentation_model(images)#(images, output_seg)#(output_final)\n",
    "\n",
    "            loss = criterion(seg, outputs)\n",
    "\n",
    "            outputs = torch.nn.functional.softmax(outputs, dim = 1)\n",
    "\n",
    "            tp = f1_loss(seg, outputs)#iou_pytorch(seg, output_final)\n",
    "\n",
    "            iou = iou_pytorch(seg, outputs)\n",
    "\n",
    "            #loss = loss+tp\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), VAL_F1_Loss = tp.item(), IOU = iou.item())#, Val_Accuracy = accuracy, Val_F1_Score = f1) #, VAL_F1_Loss = tp.item(), IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n",
    "\n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "\n",
    "                checkpoint_vit = {\n",
    "                                'model_state_dict': segmentation_model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint_vit, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_test_for_seg_new_2_tr.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"###### Epoch: \", epoch,\"_\",val_acc_mean/15,\" ######\")\n",
    "        #plt.imshow(torch.argmax(output_final, dim = 1)[0].detach().cpu().numpy())\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Segmentation\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义分割模型\n",
    "class SegmentationModel(nn.Module):\n",
    "    def __init__(self, model_name, num_classes):\n",
    "        super(SegmentationModel, self).__init__()\n",
    "        # 创建timm模型\n",
    "        model = timm.create_model(model_name, pretrained=True)\n",
    "\n",
    "        # 去除最后的全局平均池化层和全连接层\n",
    "        self.backbone = nn.Sequential(*list(model.children())[:-2])\n",
    "\n",
    "        # 获取num_features\n",
    "        num_features = model.layer4[2].conv3.out_channels\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(num_features, 512, kernel_size=7, padding=3, output_padding=0, stride=4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=8, padding=1, output_padding=0, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=8, padding=1, output_padding=0, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ConvTranspose2d(128, num_classes, kernel_size=8, padding=3, output_padding=0, stride=2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.segmentation_head(x)\n",
    "        return x\n",
    "\n",
    "# 指定模型名称和类别数\n",
    "model_name = 'resnet50'  # 选择适合你任务的模型，可以根据需要更改\n",
    "num_classes = 2  # 类别数为2\n",
    "\n",
    "# 创建分割模型\n",
    "segmentation_model = SegmentationModel(model_name, num_classes).to(device)\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "\n",
    "\n",
    "number_of_epochs = 10000\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"Unet_Version\"\n",
    "\n",
    "#ckpt_path = os.path.join(\"Model_Ckpts\",runs+f\"ckpt_2.pt\")\n",
    "\n",
    "ckpt_for_tr = r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\Model_Ckpts\\Unet_Versionckpt_test_for_seg_new_2_tr.pth\" #os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\")\n",
    "\n",
    "checkpoint = torch.load(ckpt_for_tr)\n",
    "\n",
    "#cls_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#cls_model.train()\n",
    "\n",
    "#model_vit.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "segmentation_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(segmentation_model.parameters(), lr=1e-3)#, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.MSELoss() #nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "segmentation_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(valloader)\n",
    "        val_loss = 0\n",
    "        val_acc_mean = 0\n",
    "\n",
    "        #cls_model.eval()\n",
    "\n",
    "\n",
    "\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "            outputs = segmentation_model(images)#(images, output_seg)#(output_final)\n",
    "\n",
    "            loss = criterion(seg, outputs)\n",
    "\n",
    "            outputs = torch.nn.functional.softmax(outputs, dim = 1)\n",
    "\n",
    "            tp = f1_loss(seg, outputs)#iou_pytorch(seg, output_final)\n",
    "\n",
    "            iou = iou_pytorch(seg, outputs)\n",
    "\n",
    "            #loss = loss+tp\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), VAL_F1_Loss = tp.item(), IOU = iou.item())#, Val_Accuracy = accuracy, Val_F1_Score = f1) #, VAL_F1_Loss = tp.item(), IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seg_real = torch.argmax(seg, dim = 1)\n",
    "outputs_real = torch.argmax(outputs, dim=1)\n",
    "\n",
    "for i in range(seg_real.shape[0]):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(seg_real[i].detach().cpu().numpy())\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(outputs_real[i].detach().cpu().numpy())\n",
    "\n",
    "    plt.figure()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Drivers Classification\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "\n",
    "\n",
    "number_of_epochs = 10000\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"Unet_Version\"\n",
    "\n",
    "#ckpt_path = os.path.join(\"Model_Ckpts\",runs+f\"ckpt_2.pt\")\n",
    "\n",
    "ckpt_for_tr = r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\Model_Ckpts\\Unet_Versionckpt_test_less_lr_model_optimizer_scheduler_checkpoint_new.pth\" #os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\")\n",
    "\n",
    "checkpoint = torch.load(ckpt_for_tr)\n",
    "\n",
    "#cls_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#cls_model.train()\n",
    "\n",
    "#model_vit.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model_vit.parameters(), lr=1e-4)#, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 51, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "#model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    pbar = tqdm(trainloader)\n",
    "    train_loss = 0\n",
    "    acc = 0\n",
    "    model_vit.train()\n",
    "\n",
    "    for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      zero_input = torch.empty(seg.shape[0], 1, seg.shape[2], seg.shape[-1])\n",
    "\n",
    "      add_zero = torch.zeros_like(zero_input).to(device)\n",
    "\n",
    "      output_seg = torch.concat((seg, add_zero), dim = 1)\n",
    "\n",
    "      output_final = images + output_seg\n",
    "\n",
    "      outputs = model_vit(images)#(images, output_seg)#(output_final)\n",
    "\n",
    "      loss = criterion(labels, outputs)\n",
    "\n",
    "      #tp = f1_loss(seg, output_final) #iou_pytorch(seg, output_final)\n",
    "\n",
    "      #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "      #loss = loss+tp\n",
    "\n",
    "      outputs = torch.argmax(outputs,dim=-1).squeeze()\n",
    "      labels = torch.argmax(labels,dim=-1).squeeze()\n",
    "\n",
    "      correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "      total = labels.size(0)\n",
    "\n",
    "\n",
    "      accuracy = correct / total\n",
    "\n",
    "      pbar.set_postfix(Train_Loss=loss.item(), Train_Accuracy = accuracy)#, Train_F1_Loss = tp.item(), IOU =iou.item())\n",
    "\n",
    "      train_loss += loss\n",
    "\n",
    "\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      '''\n",
    "      if train_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = train_loss\n",
    "\n",
    "\n",
    "                checkpoint = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "      '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(valloader)\n",
    "        val_loss = 0\n",
    "        val_acc_mean = 0\n",
    "\n",
    "        #cls_model.eval()\n",
    "\n",
    "        model_vit.eval()\n",
    "\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "            zero_input = torch.empty(seg.shape[0], 1, seg.shape[2], seg.shape[-1])\n",
    "\n",
    "            add_zero = torch.zeros_like(zero_input).to(device)\n",
    "\n",
    "            output_seg = torch.concat((seg, add_zero), dim = 1)\n",
    "\n",
    "            output_final = images + output_seg\n",
    "\n",
    "            outputs = model_vit(images)#(images, output_seg)#(output_final)\n",
    "\n",
    "            loss = criterion(labels, outputs)\n",
    "\n",
    "            #tp = f1_loss(seg, output_final)#iou_pytorch(seg, output_final)\n",
    "\n",
    "            #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "            #loss = loss+tp\n",
    "\n",
    "            outputs = torch.argmax(outputs,dim=-1).squeeze()\n",
    "            labels = torch.argmax(labels,dim=-1).squeeze()\n",
    "\n",
    "            correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "            total = labels.size(0)\n",
    "\n",
    "\n",
    "\n",
    "            accuracy = correct / total\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), Val_Accuracy = accuracy)#, VAL_F1_Loss = tp.item(), IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n",
    "\n",
    "            val_acc_mean += accuracy\n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "\n",
    "                checkpoint_vit = {\n",
    "                                'model_state_dict': model_vit.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),}\n",
    "                                #'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                #}\n",
    "\n",
    "\n",
    "                torch.save(checkpoint_vit, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_test_less_lr_model_optimizer_scheduler_checkpoint_new_seperate.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"###### Epoch: \", epoch,\"_\",val_acc_mean/15,\" ######\")\n",
    "        #plt.imshow(torch.argmax(output_final, dim = 1)[0].detach().cpu().numpy())\n",
    "\n",
    "    #scheduler.step()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Drivers Classification\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "\n",
    "\n",
    "number_of_epochs = 10000\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"Unet_Version\"\n",
    "\n",
    "#ckpt_path = os.path.join(\"Model_Ckpts\",runs+f\"ckpt_2.pt\")\n",
    "\n",
    "ckpt_for_tr = r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\Model_Ckpts\\Unet_Versionckpt_test_less_lr_model_optimizer_scheduler_checkpoint_new.pth\" #os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\")\n",
    "\n",
    "checkpoint = torch.load(ckpt_for_tr)\n",
    "\n",
    "#cls_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#cls_model.train()\n",
    "\n",
    "#model_vit.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model_vit.parameters(), lr=1e-4)#, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 51, eta_min=0, last_epoch=-1, verbose=False)\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "#model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    pbar = tqdm(trainloader)\n",
    "    train_loss = 0\n",
    "    acc = 0\n",
    "    model_vit.train()\n",
    "\n",
    "    for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      zero_input = torch.empty(seg.shape[0], 1, seg.shape[2], seg.shape[-1])\n",
    "\n",
    "      add_zero = torch.zeros_like(zero_input).to(device)\n",
    "\n",
    "      output_seg = torch.concat((seg, add_zero), dim = 1)\n",
    "\n",
    "      output_final = images + output_seg\n",
    "\n",
    "      outputs = model_vit(images)#(images, output_seg)#(output_final)\n",
    "\n",
    "      loss = criterion(labels, outputs)\n",
    "\n",
    "      #tp = f1_loss(seg, output_final) #iou_pytorch(seg, output_final)\n",
    "\n",
    "      #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "      #loss = loss+tp\n",
    "\n",
    "      outputs = torch.argmax(outputs,dim=-1).squeeze()\n",
    "      labels = torch.argmax(labels,dim=-1).squeeze()\n",
    "\n",
    "      correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "      total = labels.size(0)\n",
    "\n",
    "\n",
    "      accuracy = correct / total\n",
    "\n",
    "      pbar.set_postfix(Train_Loss=loss.item(), Train_Accuracy = accuracy)#, Train_F1_Loss = tp.item(), IOU =iou.item())\n",
    "\n",
    "      train_loss += loss\n",
    "\n",
    "\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      '''\n",
    "      if train_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = train_loss\n",
    "\n",
    "\n",
    "                checkpoint = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "      '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(valloader)\n",
    "        val_loss = 0\n",
    "        val_acc_mean = 0\n",
    "\n",
    "        #cls_model.eval()\n",
    "\n",
    "        model_vit.eval()\n",
    "\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "            zero_input = torch.empty(seg.shape[0], 1, seg.shape[2], seg.shape[-1])\n",
    "\n",
    "            add_zero = torch.zeros_like(zero_input).to(device)\n",
    "\n",
    "            output_seg = torch.concat((seg, add_zero), dim = 1)\n",
    "\n",
    "            output_final = images + output_seg\n",
    "\n",
    "            outputs = model_vit(images)#(images, output_seg)#(output_final)\n",
    "\n",
    "            loss = criterion(labels, outputs)\n",
    "\n",
    "            #tp = f1_loss(seg, output_final)#iou_pytorch(seg, output_final)\n",
    "\n",
    "            #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "            #loss = loss+tp\n",
    "\n",
    "            outputs = torch.argmax(outputs,dim=-1).squeeze()\n",
    "            labels = torch.argmax(labels,dim=-1).squeeze()\n",
    "\n",
    "            correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "            total = labels.size(0)\n",
    "\n",
    "\n",
    "\n",
    "            accuracy = correct / total\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), Val_Accuracy = accuracy)#, VAL_F1_Loss = tp.item(), IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n",
    "\n",
    "            val_acc_mean += accuracy\n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "\n",
    "                checkpoint_vit = {\n",
    "                                'model_state_dict': model_vit.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),}\n",
    "                                #'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                #}\n",
    "\n",
    "\n",
    "                torch.save(checkpoint_vit, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_test_less_lr_model_optimizer_scheduler_checkpoint_new_seperate.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"###### Epoch: \", epoch,\"_\",val_acc_mean/15,\" ######\")\n",
    "        #plt.imshow(torch.argmax(output_final, dim = 1)[0].detach().cpu().numpy())\n",
    "\n",
    "    #scheduler.step()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Drivers Classification\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "pretrained_resnet_18 = timm.create_model('resnet18', pretrained=True)\n",
    "num_ftrs = pretrained_resnet_18.get_classifier().in_features\n",
    "pretrained_resnet_18.fc = nn.Linear(num_ftrs, 4)\n",
    "pretrained_resnet_18 = nn.Sequential(\n",
    "    pretrained_resnet_18,\n",
    "    nn.Softmax(dim=1)  # 在第一维度（列）上应用softmax\n",
    ")\n",
    "\n",
    "pretrained_resnet_18 = pretrained_resnet_18.to(device)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "\n",
    "\n",
    "#seg_optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)#, weight_decay=1e-5)\n",
    "#seg_criterion = nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "#seg_scheduler = torch.optim.lr_scheduler.MultiStepLR(seg_optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "number_of_epochs = 10000\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"Unet_Version\"\n",
    "\n",
    "#ckpt_path = os.path.join(\"Model_Ckpts\",runs+f\"ckpt_2.pt\")\n",
    "\n",
    "\n",
    "#ckpt_for_tr = r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\Model_Ckpts\\Unet_Versionckpt_test_less_lr_model_optimizer_scheduler_checkpoint_pretrained_resnet_18.pth\" #os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\")\n",
    "\n",
    "#checkpoint = torch.load(ckpt_for_tr)\n",
    "\n",
    "#pretrained_resnet_18.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#seg_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#seg_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "\n",
    "\n",
    "pretrained_resnet_18.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(pretrained_resnet_18.parameters(), lr=1e-3)#, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "#optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#seg_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "#model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    pbar = tqdm(trainloader)\n",
    "    train_loss = 0\n",
    "    acc = 0\n",
    "    pretrained_resnet_18.train()\n",
    "    for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      #model.eval()\n",
    "\n",
    "      #with torch.no_grad():\n",
    "      #output_final = model(images)\n",
    "\n",
    "      outputs = pretrained_resnet_18(images)#(output_final)\n",
    "\n",
    "      loss = criterion(labels.type(torch.float32), outputs.type(torch.float32))\n",
    "\n",
    "      tp = f1_loss(labels, outputs) #iou_pytorch(seg, output_final)\n",
    "\n",
    "      #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "      #loss = loss+tp\n",
    "\n",
    "      outputs = torch.argmax(outputs,dim=-1).squeeze()\n",
    "      labels = torch.argmax(labels,dim=-1).squeeze()\n",
    "\n",
    "      correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "      total = labels.size(0)\n",
    "\n",
    "\n",
    "      accuracy = correct / total\n",
    "\n",
    "      pbar.set_postfix(Train_Loss=loss.item(), Train_Accuracy = accuracy, Train_F1_Loss = tp.item())#, IOU =iou.item())\n",
    "\n",
    "      train_loss += loss\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      '''\n",
    "      if train_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = train_loss\n",
    "\n",
    "\n",
    "                checkpoint = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "      '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(valloader)\n",
    "        val_loss = 0\n",
    "        val_acc_mean = 0\n",
    "        #model.eval()\n",
    "        pretrained_resnet_18.eval()\n",
    "\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "            #with torch.no_grad():\n",
    "            #output_final = model(images)\n",
    "\n",
    "            outputs = pretrained_resnet_18(images)#(output_final)\n",
    "\n",
    "            loss = criterion(labels.type(torch.float32), outputs.type(torch.float32))\n",
    "\n",
    "            tp = f1_loss(labels, outputs)\n",
    "\n",
    "\n",
    "            #tp = f1_loss(seg, output_final)#iou_pytorch(seg, output_final)\n",
    "\n",
    "            #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "            #loss = loss+tp\n",
    "\n",
    "            outputs = torch.argmax(outputs,dim=-1).squeeze()\n",
    "            labels = torch.argmax(labels,dim=-1).squeeze()\n",
    "\n",
    "            correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "            total = labels.size(0)\n",
    "\n",
    "\n",
    "\n",
    "            accuracy = correct / total\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), Val_Accuracy = accuracy, VAL_F1_Loss = tp.item())#, IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n",
    "\n",
    "            val_acc_mean += accuracy\n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "\n",
    "                checkpoint_vit = {\n",
    "                                'model_state_dict': pretrained_resnet_18.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                #'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint_vit, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_test_less_lr_model_optimizer_scheduler_checkpoint_pretrained_resnet_18_2.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"###### Epoch: \", epoch,\"_\",val_acc_mean/15,\" ######\")\n",
    "        #plt.imshow(torch.argmax(output_final, dim = 1)[0].detach().cpu().numpy())\n",
    "\n",
    "    #scheduler.step()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# For Drivers Classification\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "BATCH_SIZE = 768\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "\n",
    "\n",
    "#seg_optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)#, weight_decay=1e-5)\n",
    "#seg_criterion = nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "#seg_scheduler = torch.optim.lr_scheduler.MultiStepLR(seg_optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "number_of_epochs = 10000\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"Unet_Version\"\n",
    "\n",
    "#ckpt_path = os.path.join(\"Model_Ckpts\",runs+f\"ckpt_2.pt\")\n",
    "\n",
    "\n",
    "#ckpt_for_tr = r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\Model_Ckpts\\Unet_Versionckpt_test_less_lr_model_optimizer_scheduler_checkpoint.pth\" #os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\")\n",
    "\n",
    "#checkpoint = torch.load(ckpt_for_tr)\n",
    "\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "#seg_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "#seg_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "\n",
    "\n",
    "model_vit.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model_vit.parameters(), lr=1e-3)#, weight_decay=1e-5)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "#model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    pbar = tqdm(trainloader)\n",
    "    train_loss = 0\n",
    "    acc = 0\n",
    "    model.eval()\n",
    "    model_vit.train()\n",
    "    for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      #model.eval()\n",
    "\n",
    "      #with torch.no_grad():\n",
    "      #output_final = model(images)\n",
    "\n",
    "      outputs = model_vit(images)#(output_final)\n",
    "\n",
    "      loss = criterion(labels.type(torch.float32), outputs.type(torch.float32))\n",
    "\n",
    "      #tp = f1_loss(seg, output_final) #iou_pytorch(seg, output_final)\n",
    "\n",
    "      #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "      #loss = loss+tp\n",
    "\n",
    "      outputs = torch.argmax(outputs,dim=-1).squeeze()\n",
    "      labels = torch.argmax(labels,dim=-1).squeeze()\n",
    "\n",
    "      correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "      total = labels.size(0)\n",
    "\n",
    "\n",
    "      accuracy = correct / total\n",
    "\n",
    "      pbar.set_postfix(Train_Loss=loss.item(), Train_Accuracy = accuracy)#, Train_F1_Loss = tp.item(), IOU =iou.item())\n",
    "\n",
    "      train_loss += loss\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      '''\n",
    "      if train_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = train_loss\n",
    "\n",
    "\n",
    "                checkpoint = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "      '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(valloader)\n",
    "        val_loss = 0\n",
    "        val_acc_mean = 0\n",
    "        #model.eval()\n",
    "        model_vit.eval()\n",
    "\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "            #with torch.no_grad():\n",
    "            #output_final = model(images)\n",
    "\n",
    "            outputs = model_vit(images)#(output_final)\n",
    "\n",
    "            loss = criterion(labels.type(torch.float32), outputs.type(torch.float32))\n",
    "\n",
    "\n",
    "            #tp = f1_loss(seg, output_final)#iou_pytorch(seg, output_final)\n",
    "\n",
    "            #iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "            #loss = loss+tp\n",
    "\n",
    "            outputs = torch.argmax(outputs,dim=-1).squeeze()\n",
    "            labels = torch.argmax(labels,dim=-1).squeeze()\n",
    "\n",
    "            correct = (outputs == labels).sum().item()\n",
    "\n",
    "\n",
    "            total = labels.size(0)\n",
    "\n",
    "\n",
    "\n",
    "            accuracy = correct / total\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), Val_Accuracy = accuracy)#, VAL_F1_Loss = tp.item(), IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n",
    "\n",
    "            val_acc_mean += accuracy\n",
    "\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "\n",
    "                checkpoint_vit = {\n",
    "                                'model_state_dict': model_vit.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                #'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint_vit, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_test_less_lr_model_optimizer_scheduler_checkpoint_vit_only.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "\n",
    "\n",
    "\n",
    "        print(\"###### Epoch: \", epoch,\"_\",val_acc_mean/15,\" ######\")\n",
    "        #plt.imshow(torch.argmax(output_final, dim = 1)[0].detach().cpu().numpy())\n",
    "\n",
    "    #scheduler.step()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#from transformers import Mask2FormerConfig, AutoImageProcessor, Mask2FormerForUniversalSegmentation\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "#configuration = Mask2FormerConfig()\n",
    "# load Mask2Former fine-tuned on COCO panoptic segmentation\n",
    "#processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\")\n",
    "#Feature_Extraction_Model = Mask2FormerForUniversalSegmentation(configuration).from_pretrained(\"facebook/mask2former-swin-base-coco-panoptic\").to(device)\n",
    "#Feature_Extraction_Model.eval()\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_dataset = ForestDataset(csv_file_to_df=train_df, transform=transform)\n",
    "val_dataset = ForestDataset(csv_file_to_df=val_df, transform=transform)\n",
    "test_dataset = ForestDataset(csv_file_to_df=test_df, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE,shuffle=False)\n",
    "dataloaders = {'train': trainloader, 'val':valloader}\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)#, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()#BCEDiceLoss(eps=1e-7, activation=None)#nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100,200,300,500], gamma=0.1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "number_of_epochs = 10000\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "runs = \"Unet_Version\"\n",
    "\n",
    "#ckpt_path = os.path.join(\"Model_Ckpts\",runs+f\"ckpt_2.pt\")\n",
    "\n",
    "ckpt_for_tr = r\"C:\\Users\\haoti\\PycharmProjects\\EECS_6998_E11\\Model_Ckpts\\Unet_Versionckpt_test_less_lr_model_optimizer_scheduler_checkpoint.pth\" #os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\")\n",
    "\n",
    "checkpoint = torch.load(ckpt_for_tr)\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "#model.eval()\n",
    "\n",
    "#model.load_state_dict(torch.load(ckpt_path))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "\n",
    "    pbar = tqdm(trainloader)\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      output_final = model(images)\n",
    "\n",
    "      loss = criterion(seg.type(torch.float32), output_final.type(torch.float32))\n",
    "\n",
    "      tp = f1_loss(seg, output_final) #iou_pytorch(seg, output_final)\n",
    "\n",
    "      iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "      loss = loss+tp\n",
    "\n",
    "      pbar.set_postfix(Train_Loss=loss.item(), Train_F1_Loss = tp.item(), IOU =iou.item())\n",
    "\n",
    "      train_loss += loss\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      '''\n",
    "      if train_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = train_loss\n",
    "\n",
    "\n",
    "                checkpoint = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr_model_optimizer_scheduler_checkpoint_tr.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "      '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        pbar = tqdm(trainloader)\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        for i, (images, labels, seg) in enumerate(pbar):\n",
    "\n",
    "            output_final = model(images)\n",
    "\n",
    "            loss = criterion(seg.type(torch.float32), output_final.type(torch.float32))\n",
    "\n",
    "            tp = f1_loss(seg, output_final)#iou_pytorch(seg, output_final)\n",
    "\n",
    "            iou = iou_pytorch(seg, output_final)\n",
    "\n",
    "            loss = loss+tp\n",
    "\n",
    "            pbar.set_postfix(Val_Loss=loss.item(), VAL_F1_Loss = tp.item(), IOU = iou.item())\n",
    "\n",
    "            val_loss += loss\n",
    "\n",
    "            '''\n",
    "            if val_loss < best_val_loss:\n",
    "\n",
    "                best_val_loss = val_loss\n",
    "\n",
    "\n",
    "                checkpoint = {\n",
    "                                'model_state_dict': model.state_dict(),\n",
    "                                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                                'scheduler_state_dict': scheduler.state_dict()\n",
    "                                                                                }\n",
    "\n",
    "\n",
    "                torch.save(checkpoint, os.path.join(\"Model_Ckpts\",runs+f\"ckpt_test_less_lr_model_optimizer_scheduler_checkpoint.pth\"))\n",
    "                #torch.save(model.state_dict(), os.path.join(\"Model_Ckpts\",runs+f\"ckpt_3_less_lr.pt\"))\n",
    "            '''\n",
    "\n",
    "        seg_real = torch.argmax(seg, dim = 1)\n",
    "        outputs_real = torch.argmax(output_final, dim=1)\n",
    "\n",
    "        for i in range(seg_real.shape[0]):\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(seg_real[i].detach().cpu().numpy())\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(outputs_real[i].detach().cpu().numpy())\n",
    "\n",
    "        print(\"###### Epoch: \", epoch,\"_\",val_loss,\" ######\")\n",
    "        seg_real = torch.argmax(seg, dim = 1)\n",
    "        outputs_real = torch.argmax(output_final, dim=1)\n",
    "\n",
    "        for i in range(seg_real.shape[0]):\n",
    "            plt.figure(figsize=(12, 4))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.imshow(seg_real[i].detach().cpu().numpy())\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.imshow(outputs_real[i].detach().cpu().numpy())\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seg_real = torch.argmax(seg, dim = 1)\n",
    "outputs_real = torch.argmax(output_final, dim=1)\n",
    "\n",
    "for i in range(seg_real.shape[0]):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(seg_real[i].detach().cpu().numpy())\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(outputs_real[i].detach().cpu().numpy())\n",
    "\n",
    "    plt.figure()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
